---
title: 线性回归与非线性回归
comment: true
date: 2021-09-11 16:57:24
tags:
categories:
addrlink:
---

# 线性回归

回归就是预测值是连续的，而线性就是一条线，这条线可以用函数公式表示出来。



## 代价函数

代价函数，Cost Function

定义：衡量模型预测出来的值 $h(\theta)$ 与真实值 $y$ 之间的差异的函数，记： $C(\theta)$ 。

这是一个样本的代价函数，当有多个样本时，所有样本的代价函数的平均值叫总的代价函数，记： $J(\theta)$ 。

$J(\theta)$ 是常用来评价模型的好坏。

> $h(\theta)$ 中的 $h$ ，其实是hypothesis的缩写，所以也叫：假设函数



在线性回归中，最常用的代价函数公式就是**均方误差（Mean squared error）** ：
$$
\begin{align}
J(\theta_0, \theta_1) & = \frac 1 {2m} \sum ^m _{i=1} (\hat{y}^{(i)}-y^{(i)})^2       \\
					  & =\frac 1 {2m} \sum ^m _{i=1}(h_{\theta}(x^{(i)})-y^{(i)})^2
\end{align}
$$
其中：

- m：训练样本的个数
- $h_{\theta}(x^{(i)})$ ：用参数$\theta$ 和第i个样本值x预测出来的y值
- y：原训练样本中的y值，也就是标准答案
- 上角标$（i）$ ：第i个样本
- $\theta_i$ ：第i个特征值





## 梯度下降算法

梯度下降，Gradient Descent

它的主要目的是通过迭代找到目标函数的最小值，或者收敛到最小值。



梯度下降算法的基本思想可类比为快速下山的过程。

假设这样一个场景：一个人被困在山上，需要从山上下来（找到山的最低点，也就是山谷）。但此时山上的浓雾很大，导致可视度很低；因此，下山的路径就无法确定，必须利用自己周围的信息一步一步地找到下山的路。这个时候，便可利用梯度下降算法来帮助自己下山。

怎么做呢，首先以他当前的所处的位置为基准，寻找这个位置**最陡峭**的地方，然后朝着下降方向走一步，然后又继续以当前位置为基准，再找最陡峭的地方，再走直到最后到达最低处；同理上山也是如此，只是这时候就变成梯度上升算法了



### 全量梯度下降算法

全量梯度下降算法，Batch Gradient Descent

使用所有的样本数据

公式：
$$
\begin{align}
\theta_j & :=\theta_j - \alpha \frac {\partial} {\partial \theta_j} J(\theta_0,\theta_1,……,\theta_m) \\
		 & :=\theta_j - \alpha \frac 1 m \sum _{i=1} ^m (h_{\theta} (x^{(i)})-y^{(i)})*x_j ^{(i)}
\end{align}
$$
其中：

- `j`：第j个特征值，$j=0,……,n$ 
- `m`：样本总数
- `:=`：表示将右边的结果赋值给左边，如：`a:=b+c`，表示将`b+c`的结果赋值给`a`
- $\alpha$ ：学习率，用来控制每一步走的距离
- $x^{(i)}$ ：第i个样本
- $x_j ^{(i)}$ ：第i个样本中的第j个特征值，注意，$x_0 ^{(i)}=1$ 



算法过程：

重复上述公式，直到$\theta_j$为最小值，注意，需要同时对所有 $\theta_j$ 进行更新。





### 矩阵表示

为了更好的计算，通常会采用矩阵的表示方式。

比如，原来的假设函数是：
$$
h_{\theta}(x)=\theta_0 x_0+ \theta_1 x_1 + \theta_2 x_2 + …… + \theta_n x_n
$$
其中，$x_0=1$

现在取$X= \begin{bmatrix} x_0 \\x_1 \\ \vdots \\ x_n \end{bmatrix}$ ，$\Theta= \begin{bmatrix} \theta_0 \\ \theta_0 \\ \vdots \\ \theta_n \end{bmatrix}$ 

所以，$h_{\theta}(x)=\Theta^{\top} X$



以此类推，

代价函数会变成：





### 特征缩放

面对特征数量较多的时候，保证这些特征具有相近的尺度，可以使梯度下降法更快的收敛。

![img](D:\blog\source\_drafts\线性回归与非线性回归\1.png)



特征缩放的四种方式：

1. min-max标准化，Min-Max Normalization，又称Rescaling：
   $$
   x^*=\frac{x-min(x)}{max(x)-min(x)}
   $$
   

2. 均值归一化，Mean Normalization：
   $$
   x^*=\frac{x-mean(x)}{max(x)-min(x)}
   $$

3. z-score标准化，Standardization：
   $$ {align}
   x^*=\frac {x-mean(x)} {std(x)} \\
   $$ {align}
   其中：
   $$
   std(x)=\sqrt {\frac{\sum {(x-mean(x))^2}} n}
   $$

4. 缩放到单位长度，Scaling to unit length：
   $$
   x^*=\frac {x} {\lVert x \rVert}
   $$
   其中，$\lVert x \rVert$ 为x向量的欧拉长度





### 学习率

当学习率太大时，随着梯度下降的迭代次数增加，代价函数的值可能会反弹，如：

<img src="D:\blog\source\_drafts\线性回归与非线性回归\2.png" alt="image-20210913111017767" style="zoom:80%;" />



<img src="D:\blog\source\_drafts\线性回归与非线性回归\3.png" alt="image-20210913111048602" style="zoom:80%;" />

<img src="D:\blog\source\_drafts\线性回归与非线性回归\4.png" alt="image-20210913111126608" style="zoom:80%;" />



这时候就需要取一个更小的学习率。

一般取法：每隔10倍取一个值，然后观察迭代曲线。








# 非线性回归

