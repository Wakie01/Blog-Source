---
title: 损失函数
comment: true
date: 2021-10-09 10:38:40
tags:
categories:
addrlink:
---



# 常见Loss Function

## 分类错误率(Classification Error)

分类错误率，CE，是最直接、最简单的损失函数，公式：
$$
classification \; error=\frac {count \; of \; error \; items}{count \; of \; all \; items}
$$


缺点：当两个模型的错误个数相同时，并不能判断哪个模型更好。

所以说，这个基本上没人用。



## 均方误差(Mean Square Error)

均方误差损失，MSE，是一种较常见、简单的损失函数，公式：
$$
MSE=\frac 1 n \sum _{i=1} ^n (\hat{y_i} - y_i)^2
$$
其中：

-  $\hat{y_i}$ 与 $y_i$ 分别是预测值和真实值
- $n$ 为测试集数量



MSE就是：真实值与预测值的差值的平方，然后求和平均。

当误差越大时，MSE就更大。



## 均方根误差(Root Mean Square Error)

均方根误差，RMSE，MSE的改进版，在MSE的基础上，加一个根号，公式：
$$
RMSE=\sqrt {\frac 1 n \sum _{i=1} ^n (\hat{y_i} - y_i)^2}
$$
其中：

-  $\hat{y_i}$ 与 $y_i$ 分别是预测值和真实值
- $n$ 为测试集数量



## 平均绝对误差(Mean Absolute Error)

平均绝对误差，MAE，也是MSE的改进版，将误差的求平方变成求绝对值，公式：
$$
MAE=\frac 1 n \sum _{i=1} ^n \lvert \hat{y_i} - y_i \rvert
$$
其中：

-  $\hat{y_i}$ 与 $y_i$ 分别是预测值和真实值
- $n$ 为测试集数量





## 交叉熵损失函数(Cross Entropy Loss Function)

在了解交叉熵损失函数之前，得先了解什么熵。

什么是 **熵** 呢？

熵常用来衡量一个系统的信息量，所以在了解熵之前，得要了解信息量



什么是**信息量**呢？

**信息量**

- 定义：信息多少的量度。

- 计算方法：香农的衡量信息量的公式：
  $$
  \begin{align}
  A的信息量 &=log _2 \frac {1}{P(A)}\\ 
  &= -log _2 P(A)
  \end{align}
  $$

- $P(A)$ ：事件A发生的概率

- $P(A)$ 越大，A的信息量越小；$P(A)$ 越小，A的信息量就越大。

- 案例：如果我说，太阳从东方升起。那么这个事件发生的概率几乎为1，那么这个事情的反应的信息量就会很小。但如果我说，太阳从西方升起。那么这就反应的信息量就很大了，这有可能是因为地球的自转变成了自东向西，或者别的原因，然后随之引起各种变化



**熵（Entropy）** ，也叫信息熵

- 定义：离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。

- 作用：描述的一个事件的不确定性。

  此外，它还常被用来作为一个系统，或者一维的信息含量的量化指标，从而可以进一步用来作为系统方程优化的目标或者参数选择的判据。

- 公式：
  $$
  \begin{align}
  H(X) &= \sum _{i=1} ^n P(i)*log_2 \frac {1}{P(i)} \\
  &=- \sum _{i=1} ^n P(i)*log_2 P(i)
  \end{align}
  $$


  - $H(X)$ ：系统X的信息含量，系统X有n个事件
  - $P(i)$ ：事件i的发生概率

- 理解：信息熵就相当于：对所有可能发生的事件，所产生的信息量的期望。





此外还有**联合熵**、**条件熵** 、**交叉熵** 、

**联合熵（Joint Entropy）**

- 刚刚的信息熵，可以看成是一维的： $P(i)$ ，随机变量只有一个：$X$ 。

- 联合熵就是对信息熵的升级：将一维变成多维： $P(i,j)$ ，将一个随机变量变成多个： $X,Y$ 。

- 公式：
  $$
  H(X,Y)=-\sum _{i=1} ^n \sum _{j=1} ^m P(x_i,y_j) * log_2 P(x_i,y_j)
  $$
  三维就继续往上加就是了：
  $$
  H(X,Y,Z)=-\sum _{i=1} ^n \sum _{j=1} ^m \sum _{k=1} ^l P(x_i,y_j,z_k) * log_2 P(x_i,y_j,z_k)
  $$
  



**条件熵（Conditional Entropy）**

- 条件熵：$H(Y|X)$ ，表示，在随机变量X的条件下，随机变量Y的信息熵。

- 公式：
  $$
  H(Y|X)=
  $$
  





**交叉熵（Cross Entropy）**











# 总结

在线性回归问题上，常用MSE、RMSE、MAE作为损失函数

而在分类问题上，则常用CELF作为损失函数

