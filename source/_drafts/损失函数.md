---
title: 损失函数
comment: true
date: 2021-10-09 10:38:40
tags:
categories:
addrlink:
---



# 常见Loss Function

## 分类错误率(Classification Error)

分类错误率，CE，是最直接、最简单的损失函数，公式：
$$
classification \; error=\frac {count \; of \; error \; items}{count \; of \; all \; items}
$$


缺点：当两个模型的错误个数相同时，并不能判断哪个模型更好。

所以说，这个基本上没人用。



## 均方误差(Mean Square Error)

均方误差损失，MSE，是一种较常见、简单的损失函数，公式：
$$
MSE=\frac 1 n \sum _{i=1} ^n (\hat{y_i} - y_i)^2
$$
其中：

-  $\hat{y_i}$ 与 $y_i$ 分别是预测值和真实值
- $n$ 为测试集数量



MSE就是：真实值与预测值的差值的平方，然后求和平均。

当误差越大时，MSE就更大。



## 均方根误差(Root Mean Square Error)

均方根误差，RMSE，MSE的改进版，在MSE的基础上，加一个根号，公式：
$$
RMSE=\sqrt {\frac 1 n \sum _{i=1} ^n (\hat{y_i} - y_i)^2}
$$
其中：

-  $\hat{y_i}$ 与 $y_i$ 分别是预测值和真实值
- $n$ 为测试集数量



## 平均绝对误差(Mean Absolute Error)

平均绝对误差，MAE，也是MSE的改进版，将误差的求平方变成求绝对值，公式：
$$
MAE=\frac 1 n \sum _{i=1} ^n \lvert \hat{y_i} - y_i \rvert
$$
其中：

-  $\hat{y_i}$ 与 $y_i$ 分别是预测值和真实值
- $n$ 为测试集数量





## 交叉熵损失函数(Cross Entropy Loss Function)

在了解交叉熵损失函数之前，得先了解什么熵。

什么是 **熵** 呢？

熵常用来衡量一个系统的信息量，所以在了解熵之前，得要了解信息量



什么是**信息量**呢？

**信息量**

- 定义：信息多少的量度。

- 计算方法：香农的衡量信息量的公式：
  $$
  \begin{align}
  A的信息量 &=log _2 \frac {1}{P(A)}\\ 
  &= -log _2 P(A)
  \end{align}
  $$

- $P(A)$ ：事件A发生的概率

- $P(A)$ 越大，A的信息量越小；$P(A)$ 越小，A的信息量就越大。

- 案例：如果我说，太阳从东方升起。那么这个事件发生的概率几乎为1，那么这个事情的反应的信息量就会很小。但如果我说，太阳从西方升起。那么这就反应的信息量就很大了，这有可能是因为地球的自转变成了自东向西，或者别的原因，然后随之引起各种变化

  > 此外，关于信息量的公式应满足：
  > $$
  > \begin{align}
  > f(x) & := 信息量 \\
  > f(x_1 \cdot x_2) & =f(x_1) + f(x_2)
  > \end{align}
  > $$
  >
  > - $x_1 , x_2$ ：信息
  > - $:=$ ：将右边定义为左边的符号
  >
  > 为此，香农将 $f(x)$ 定义为对数函数： $f(x) :=k \cdot log_m x$
  > $$
  > log_m(A \cdot B)=log_mA+log_mB
  > $$
  > 然后就是系数 $k$ 和 $m$ 的确定了，由于x越大，f(x)越小，所以取k=-1，而至于m，越简单越好，取m=2即可。

  



**熵（Entropy）** ，也叫信息熵

- 定义：离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。

- 作用：描述的一个系统中的所有事件的不确定性。

  此外，它还常被用来作为一个系统，或者一维的信息含量的量化指标，从而可以进一步用来作为系统方程优化的目标或者参数选择的判据。

- 公式：
  $$
  \begin{align}
  H(X) &= \sum _{i=1} ^n P(i)*log_2 \frac {1}{P(i)} \\
  &=- \sum _{i=1} ^n P(i)*log_2 P(i)
  \end{align}
  $$


  - $H(X)$ ：系统X的信息含量，系统X有n个事件
  - $P(i)$ ：事件i的发生概率
- 理解：信息熵就相当于：对所有可能发生的事件，所产生的信息量的期望。
- 对比信息量：信息量衡量的是某一个事件，而熵衡量的是某一个系统中的所有事件。









此外还有**联合熵**、**相对熵**、**条件熵** 、**交叉熵** 、

**联合熵（Joint Entropy）**

- 刚刚的信息熵，可以看成是一维的： $P(i)$ ，随机变量只有一个：$X$ 。

- 联合熵就是对信息熵的升级：将一维变成多维： $P(i,j)$ ，将一个随机变量变成多个： $X,Y$ 。

- 公式：
  $$
  H(X,Y)=-\sum _{i=1} ^n \sum _{j=1} ^m P(x_i,y_j) * log_2 P(x_i,y_j)
  $$
  三维就继续往上加就是了：
  $$
  H(X,Y,Z)=-\sum _{i=1} ^n \sum _{j=1} ^m \sum _{k=1} ^l P(x_i,y_j,z_k) * log_2 P(x_i,y_j,z_k)
  $$
  



**相对熵（Relative Entropy）**

- 相对熵，又称KL散度（Kullback-Leibler散度）

- 公式：
  $$
  \begin{align}
  D_{KL}(P || Q) & := \sum_{i=1}^m p_i \cdot \left( f_Q(q_i)-f_P(p_i) \right) \\
  & \\
  & =\sum_{i=1}^m p_i \cdot \left( (-log_2 \, q_i)-(-log_2 \, p_i) \right) \\
  & \\
  & =\sum_{i=1}^m p_i \cdot (-log_2 \, q_i) -\sum_{i=1}^m p_i \cdot (-log_2 \, p_i)
  \end{align}
  $$

  - $P，Q$ ：分别代表两个系统，这两个系统中各有m个事件，$p_i , q_i$ 分别表示P系统和Q系统中第i个事件的发生概率 
  - $D_{KL}(P || Q)$ ：已P为基准的相对熵，去考虑P和Q相差多少信息量

- 由公式中的第三条式子可见，后半部分是P的信息熵 $H(P)$ ，前半部分是P的交叉熵 $H(P,Q)$ 

- 由公式可见，当 $D_{KL}(P || Q)$ 越接近0时，P与Q就越相似

- 从吉布斯不等式可知，  $D_{KL}(P || Q) \geq 0$  ，当P与Q越相似时， $D_{KL}(P || Q)$ 就越接近0

- 因此，若要使Q系统尽可能地相似与P系统，只需使前半部分即P的交叉熵 $H(P,Q)$ 尽可能地为0即可





**交叉熵（Cross Entropy）**







**条件熵（Conditional Entropy）**

- 条件熵：$H(Y|X)$ ，表示，在随机变量X的条件下，随机变量Y的信息熵。

- 公式：
  $$
  H(Y|X)=
  $$
  

















# 总结

在线性回归问题上，常用MSE、RMSE、MAE作为损失函数

而在分类问题上，则常用CELF作为损失函数

